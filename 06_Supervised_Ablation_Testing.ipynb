{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/courtgibson13/UofM-Course-Work-Comprehensive-Project/blob/main/06_Supervised_Ablation_Testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHf_jOR9jOca"
      },
      "source": [
        "# Import, Download, & Variable Statements"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB-Lj3yZ0B9N",
        "outputId": "bcce81f0-095e-43ca-968a-31abfca5f9b7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting shap\n",
            "  Downloading shap-0.44.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (535 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/535.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.3/535.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m535.7/535.7 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.2.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.2)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (23.2)\n",
            "Collecting slicer==0.0.7 (from shap)\n",
            "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.58.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.41.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2023.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->shap) (1.16.0)\n",
            "Installing collected packages: slicer, shap\n",
            "Successfully installed shap-0.44.1 slicer-0.0.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic==1.8.2 typing-extensions==4.0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQD1IrVP1hrr",
        "outputId": "0c630b64-c7c2-4712-aaff-ed25466c78dc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic==1.8.2\n",
            "  Downloading pydantic-1.8.2-py3-none-any.whl (126 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/126.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m122.9/126.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions==4.0.1\n",
            "  Downloading typing_extensions-4.0.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: typing-extensions, pydantic\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.1\n",
            "    Uninstalling pydantic-2.6.1:\n",
            "      Successfully uninstalled pydantic-2.6.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "sqlalchemy 2.0.27 requires typing-extensions>=4.6.0, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "arviz 0.15.1 requires typing-extensions>=4.1.0, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "chex 0.1.85 requires typing-extensions>=4.2.0, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "flax 0.8.1 requires typing-extensions>=4.2, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "ibis-framework 7.1.0 requires typing-extensions<5,>=4.3.0, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "inflect 7.0.0 requires pydantic>=1.9.1, but you have pydantic 1.8.2 which is incompatible.\n",
            "librosa 0.10.1 requires typing-extensions>=4.1.1, but you have typing-extensions 4.0.1 which is incompatible.\n",
            "pydantic-core 2.16.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-1.8.2 typing-extensions-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvZ0Yn0E1j1g",
        "outputId": "77371b0a-773f-4618-c6db-1ca93eebc4b1"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (23.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.1.8->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6WzZ3_ujTwL",
        "outputId": "b7701342-06d0-486a-f3fd-f14c125ac7a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Import & download statements\n",
        "# General Statements\n",
        "#!git clone https://github.com/d-atallah/implicit_gender_bias.git\n",
        "#! pip install joblib\n",
        "#! pip install shap\n",
        "import pandas as pd\n",
        "import string\n",
        "import re\n",
        "import joblib\n",
        "#from implicit_gender_bias import config as cf\n",
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import spacy\n",
        "import shap\n",
        "import scipy\n",
        "\n",
        "\n",
        "# Feature selection & Model tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, StratifiedKFold, cross_validate\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.decomposition import TruncatedSVD,PCA, NMF\n",
        "from sklearn.metrics import confusion_matrix,precision_score, recall_score, f1_score, accuracy_score, roc_curve, roc_auc_score, log_loss, make_scorer, average_precision_score\n",
        "\n",
        "# Model options\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# NLTK resources\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter = PorterStemmer()\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ye2rxKTw0GYd",
        "outputId": "6adef4c6-fb61-4d08-a47c-0bd065217052"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLgUwfU2z_4u"
      },
      "source": [
        "## Read Inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "pPZ-eni9oS-A"
      },
      "outputs": [],
      "source": [
        "# Variables\n",
        "folder_path = '/content/drive/MyDrive/Supervised Learning Notebooks/'#'/home/gibsonce/datallah-jaymefis-gibsonce/'\n",
        "\n",
        "# Load DataFrames from pkl files\n",
        "X_train = pd.read_pickle(folder_path + 'X_train_preprocessed.pkl')\n",
        "X_test = pd.read_pickle(folder_path + 'X_test_preprocessed.pkl')\n",
        "y_train = pd.read_pickle(folder_path + 'y_train.pkl')\n",
        "y_test = pd.read_pickle(folder_path + 'y_test.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Uwy5nbK3z_4v"
      },
      "outputs": [],
      "source": [
        "non_nan_indices_train = ~X_train.isnull()\n",
        "non_nan_indices_test = ~X_test.isnull()\n",
        "\n",
        "# Filter y_train and y_test using the non-NaN indices\n",
        "y_train = y_train[non_nan_indices_train]\n",
        "y_test = y_test[non_nan_indices_test]\n",
        "\n",
        "# Filter X_train and X_test to remove NaN records\n",
        "X_train = X_train[non_nan_indices_train]\n",
        "X_test = X_test[non_nan_indices_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zRF7xFVjBKo"
      },
      "source": [
        "## Define Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_categorize(text):\n",
        "    # Tokenize and process the text using spaCy\n",
        "    doc = nlp(text)\n",
        "    word_features = ' '.join([token.text for token in doc])\n",
        "    pos_tags = ' '.join([token.pos_ for token in doc])\n",
        "\n",
        "    return word_features, pos_tags"
      ],
      "metadata": {
        "id": "jWx_7KffOvfS"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_testing(X_train, y_train, X_test, y_test, params):\n",
        "    \"\"\"\n",
        "    Runs a specified model and dimensionality reduction method with tuned hyperparameters\n",
        "\n",
        "    Parameters:\n",
        "    - X_train (array-like): Training set features, preprocessed.\n",
        "    - y_train (array-like): Training set labels.\n",
        "    - X_test (array-like): Test set features, preprocessed.\n",
        "    - y_test (array-like): Test set labels.\n",
        "    - model_type (str): Type of model to test. Choose from 'log' (Logistic Regression), 'xgb' (XGBoost), 'knn' (k-Nearest Neighbors), 'svm' (Support Vector Machine).\n",
        "    - vectorizer (str): Type of vectorizer to test. Choose from 'count' (Count Vecotizer) or 'tfidf' (TF-IDF Vecotizer).\n",
        "    - ngram (int): Feature representation to test. Choose 1 for unigrams, 2 for bigrams, and so on.\n",
        "    - params (dict): Hyperparameter grid for the specified model and dimensionality reduction method.\n",
        "\n",
        "    Returns:\n",
        "    - Pipeline: Trained and fit pipeline with the best hyperparameters.\n",
        "    - X_train_ (array-like): Preprocessed  and vectorized training set features.\n",
        "    - X_test_ (array-like): Preprocessed  and vectorized test set features.\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    X_train_word_features, X_train_pos_tags = zip(*map(tokenize_and_categorize, X_train))\n",
        "    X_test_word_features, X_test_pos_tags = zip(*map(tokenize_and_categorize, X_test))\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Spacy tokenization completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    vect = TfidfVectorizer(ngram_range=(1, 1))\n",
        "    X_train_ = vect.fit_transform(X_train)\n",
        "    X_test_ = vect.transform(X_test)\n",
        "\n",
        "    # Initialize a dictionary to store results\n",
        "    ablation_results = {}\n",
        "\n",
        "    # Iterate over unique part of speech categories\n",
        "    unique_pos_tags = np.unique([pos_tag for pos_tags in X_train_pos_tags for pos_tag in pos_tags])\n",
        "    for pos_tag in unique_pos_tags:\n",
        "        print(pos_tag)\n",
        "        # Filter out the current part of speech category\n",
        "        X_train_filtered = [doc for doc, tags in zip(X_train, X_train_pos_tags) if pos_tag not in tags]\n",
        "        X_test_filtered = [doc for doc, tags in zip(X_test, X_test_pos_tags) if pos_tag not in tags]\n",
        "\n",
        "        # Vectorize the word features\n",
        "        word_features_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "        X_train_word_features = word_features_vectorizer.fit_transform(X_train_filtered)\n",
        "        X_test_word_features = word_features_vectorizer.transform(X_test_filtered)\n",
        "\n",
        "        # Vectorize the parts of speech tags\n",
        "        pos_tags_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "        X_train_pos_tags = pos_tags_vectorizer.fit_transform(X_train_pos_tags)\n",
        "        X_test_pos_tags = pos_tags_vectorizer.transform(X_test_pos_tags)\n",
        "\n",
        "        # Combine the vectorized word features and parts of speech tags\n",
        "        X_train_combined = scipy.sparse.hstack([X_train_word_features, X_train_pos_tags])\n",
        "        X_test_combined = scipy.sparse.hstack([X_test_word_features, X_test_pos_tags])\n",
        "\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Vectorization completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "        model = XGBClassifier(random_state=42, **params.get('xgbclassifier', {}))\n",
        "        model.fit(X_train_combined, y_train)\n",
        "\n",
        "        feature_importances = model.feature_importances_\n",
        "        feature_names = word_features_vectorizer.get_feature_names_out()\n",
        "        feature_importance_dict = dict(zip(feature_names, feature_importances))\n",
        "\n",
        "        end_time = time.time()\n",
        "        elapsed_time = end_time - start_time\n",
        "        print(f\"Pipeline fitting completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "        # Store the result in the dictionary\n",
        "        ablation_results[pos_tag] = {\n",
        "            'model': model,\n",
        "            'train_features': X_train_combined,\n",
        "            'test_features': X_test_filtered_word_features_,\n",
        "            'feature_importance_dict': feature_importance_dict\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Save the trained pipeline\n",
        "    \"\"\"joblib.dump(model, f'{folder_path}{model_type}_pipeline.pkl')\n",
        "    joblib.dump(X_train_, f'{folder_path}{model_type}_X_train.pkl')\n",
        "    joblib.dump(X_test_, f'{folder_path}{model_type}_X_test.pkl')\n",
        "\n",
        "    joblib.dump(explainer, f'{folder_path}{model_type}_shap.pkl')\n",
        "    joblib.dump(feature_importance_dict, f'{folder_path}{model_type}_features.pkl')\n",
        "    print('Write to pkl file completed.')\"\"\"\n",
        "\n",
        "    return model, X_train_, X_test_, explainer, feature_importance_dict\n"
      ],
      "metadata": {
        "id": "GA1W2ZTYJv5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def model_eval(pipeline, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluates a specified model using accuracy, precision, recall, F-1 score, AUC-ROC & PR, log-Loss, and a confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - pipeline (object): Fitted pipeline.\n",
        "    - X_test (list or array): Test set features.\n",
        "    - y_test (list or array): True labels.\n",
        "\n",
        "    Returns:\n",
        "    - metrics_df (pd.DataFrame): DataFrame containing the metrics and scores.\n",
        "    - confusion_df (pd.DataFrame): DataFrame containing a confusion matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Cross-validation\n",
        "    scoring = {\n",
        "        'f1': make_scorer(f1_score),\n",
        "        'pr_auc': make_scorer(average_precision_score),\n",
        "    }\n",
        "    print('cv scoring created')\n",
        "\n",
        "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    cv_results = cross_validate(pipeline, X_test, y_test, cv=cv, scoring=scoring)\n",
        "\n",
        "    print('cv pipeline created')\n",
        "    # Create DataFrame to store cross-validation results\n",
        "    cv_metrics_df = pd.DataFrame({\n",
        "    'Metric': ['F1-Score','AUC-PR'],\n",
        "    'CV_Mean': [np.mean(cv_results['test_f1']),\n",
        "                np.mean(cv_results['test_pr_auc']),\n",
        "                ],\n",
        "    'CV_Std Dev': [np.std(cv_results['test_f1']),\n",
        "                   np.std(cv_results['test_pr_auc']),\n",
        "                  ]\n",
        "    })\n",
        "\n",
        "    print(\"\\nCross Validation:\")\n",
        "    print(cv_metrics_df)\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Cross validation completed. Time elapsed: {elapsed_time/60:.2f} minutes.\")\n",
        "\n",
        "    return cv_metrics_df"
      ],
      "metadata": {
        "id": "45e3jeJOU0rT"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsUhDyhs-EK"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG5T2udNz_4y",
        "outputId": "22c92a7e-bdfd-4c7e-d448-87286d796700"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy tokenization completed. Time elapsed: 2.48 minutes.\n",
            "Vectorization completed. Time elapsed: 2.49 minutes.\n",
            "Pipeline fitting completed. Time elapsed: 2.72 minutes.\n"
          ]
        }
      ],
      "source": [
        "# Define variables\n",
        "model = 'xgb'\n",
        "vectorization = 'tfidf'\n",
        "ngram = 1\n",
        "params = {\n",
        "    'xgbclassifier': {'subsample': 0.8, 'n_estimators': 150, 'max_depth': 9, 'learning_rate': 0.05, 'colsample_bytree': 0.5},\n",
        "    #'truncatedsvd': {'n_components': 150}\n",
        "}\n",
        "\n",
        "# Run model\n",
        "model, train, test, explainer, feature_importance_dict = model_testing(X_train, y_train, X_test, y_test, model, vectorization, ngram, params)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_word_features, X_train_pos_tags = zip(*map(tokenize_and_categorize, X_train))\n",
        "X_test_word_features, X_test_pos_tags = zip(*map(tokenize_and_categorize, X_test))"
      ],
      "metadata": {
        "id": "XM0MkEM0WM2_"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iterate over unique part of speech categories\n",
        "unique_pos_tags = [\n",
        "    'ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ',\n",
        "    'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X'\n",
        "]\n",
        "for pos_tag in unique_pos_tags:\n",
        "    print(pos_tag)\n",
        "    # Filter out the current part of speech category\n",
        "    X_train_filtered = [doc for doc, tags in zip(X_train, X_train_pos_tags) if pos_tag not in tags]\n",
        "    X_test_filtered = [doc for doc, tags in zip(X_test, X_test_pos_tags) if pos_tag not in tags]\n",
        "\n",
        "    y_train_filtered = [label for doc, label, tags in zip(X_train, y_train, X_train_pos_tags) if pos_tag not in tags]\n",
        "    y_test_filtered = [label for doc, label, tags in zip(X_test, y_test, X_test_pos_tags) if pos_tag not in tags]\n",
        "\n",
        "    # Filter the part of speech tags\n",
        "    X_train_pos_tags_filtered = [tags for tags in X_train_pos_tags if pos_tag not in tags]\n",
        "    X_test_pos_tags_filtered = [tags for tags in X_test_pos_tags if pos_tag not in tags]\n",
        "\n",
        "    # Vectorize the word features\n",
        "    word_features_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "    X_train_word_features_ = word_features_vectorizer.fit_transform(X_train_filtered)\n",
        "    X_test_word_features_ = word_features_vectorizer.transform(X_test_filtered)\n",
        "\n",
        "    # Vectorize the parts of speech tags\n",
        "    pos_tags_vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "    X_train_pos_tags_ = pos_tags_vectorizer.fit_transform(X_train_pos_tags_filtered)\n",
        "    X_test_pos_tags_ = pos_tags_vectorizer.transform(X_test_pos_tags_filtered)\n",
        "\n",
        "    # Combine the vectorized word features and parts of speech tags\n",
        "    X_train_combined = scipy.sparse.hstack([X_train_word_features_, X_train_pos_tags_])\n",
        "    X_test_combined = scipy.sparse.hstack([X_test_word_features_, X_test_pos_tags_])\n",
        "\n",
        "    model = XGBClassifier(random_state=42, **params.get('xgbclassifier', {}))\n",
        "    model.fit(X_train_combined, y_train_filtered)\n",
        "\n",
        "    metrics_df = model_eval(model, X_test_combined, y_test_filtered)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tZSPrrGNwml",
        "outputId": "9e5b15ca-e587-4aed-9f40-62c631b947a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ADJ\n",
            "cv scoring created\n",
            "cv pipeline created\n",
            "\n",
            "Cross Validation:\n",
            "     Metric   CV_Mean  CV_Std Dev\n",
            "0  F1-Score  0.540915    0.031986\n",
            "1    AUC-PR  0.527738    0.020105\n",
            "Cross validation completed. Time elapsed: 0.18 minutes.\n",
            "ADP\n",
            "cv scoring created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_test_filtered.shape)\n",
        "print(type(X_test_filtered))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "id": "q-YcBn1uVNEb",
        "outputId": "25168bd0-7b03-46f6-a922-31f065fa6a39"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'list' object has no attribute 'shape'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-8745f8da356d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_filtered\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_filtered\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "sdKBq7ETihSJ"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}